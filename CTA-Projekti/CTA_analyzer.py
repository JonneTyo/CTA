import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from dataclasses import dataclass
from itertools import product
from sklearn.decomposition import PCA


class CTA_analyzer:
    metrics = ['Test AUC', 'Sensitivity', 'Specificity', 'Accuracy']
    def __init__(self, results_dir, output_dir, **analyze_methods):
        self.results_dir = results_dir
        self.output_dir = output_dir
        self.__dict__.update(analyze_methods)
        self.analyze_methods = analyze_methods

    def __call__(self):

        for method_name, method in self.analyze_methods.items():
            method(self)
        pass

    # generates the used label based on the filename
    @staticmethod
    def get_event_from_file_name(file_name):
        label = 'event'
        label = 'cv ' + label if 'cv' in file_name else label
        label = 'timed ' + label if 'timed' in file_name else label
        return label

    # Loops through all the results in results_dir and returns them as (dataframe, filename) tuple.
    # If how='any', at least one in "require" must be in filename. Else all of them need to be found.
    # The opposite is true for require_not
    def gen_results(self, require=(), require_not=(), how='any', how_not='any'):

        if isinstance(require, str):
            require = [require]
        if isinstance(require_not, str):
            require_not = [require_not]
        for f in os.listdir(self.results_dir):

            req_list = [n in f for n in require] if len(require) > 0 else [True]
            req_not_list = [n not in f for n in require_not] if len(require_not) > 0 else [True]

            # if how and how_not == any, check if filename has at least one in require and none in require_not
            b_check = max(req_list) if how == 'any' else min(req_list)
            b_check = b_check and min(req_not_list) if how_not == 'any' else b_check and max(req_not_list)
            if b_check:
                yield pd.read_csv(self.results_dir + '\\' + f, index_col=0, header=0), f
        pass

    # Returns the values of gen_result as a dict with filenames as key and dataframe as value.
    def get_results(self, require=(), require_not=(), how='any', how_not='any'):
        to_return = {}
        for df, f in self.gen_results(require=require, require_not=require_not, how=how, how_not=how_not):
            to_return[f] = df
        return to_return

    ########################################################
    ########################################################
    # ANALYZER METHODS
    ########################################################
    ########################################################

    # Returns a dataframe with events on indexes and for columns the metrics
    # of the best model for the label. "Best" is determined by argument measure_by.
    def get_best_models(self, dfs, measure_by='sensitivity'):
        labels = ['event', 'cv event', 'timed event', 'timed cv event']
        to_return = pd.DataFrame(index=labels, columns=CTA_analyzer.metrics)
        to_return['Model'] = ""
        to_return['Model Version'] = ""
        to_return[measure_by] = -np.inf
        for df, file_name in dfs:
            label = self.get_event_from_file_name(file_name)
            for ind, row in df.iterrows():
                if to_return.at[label, measure_by] < row[measure_by]:
                    to_return.loc[label, row.index] = row
                    to_return.at[label, 'Model'] = ind
                    to_return.at[label, 'Model Version'] = file_name
        return to_return

    # Creates multiple .csv files to self.output_dir
    # Each .csv file contains values a table which shows how each setting in index affects each metric in columns
    # The file names are coded such that only the results in self.results_dir are included that have the same
    # suffixes as the .csv file at hand e.g., if the .csv file generated by this method has 'cv' in its name,
    # that means only the files in self.results_dir were included that also had 'cv' in its name.
    # 'no' keyword flips the requirement for the next keyword (the file MUST NOT have the next
    # keyword in its filename).
    def get_setting_differences(self, label_settings=('cv', 'timed'), observe_settings=('keep_cta', 'keep_pet'),
                                ignore=('only_pet',)):
        from sklearn.utils.extmath import cartesian

        def gen_file_name(label_series):
            to_return = 'settings_differences'

            for ind, val in label_series.items():
                if val == -1:
                    to_return += '_no'
                if abs(val) == 1:
                    to_return += '_' + str(ind)
            return to_return + '.csv'

        # creates kwargs dict for gen_best_models
        def create_iterator_kwargs(label_vals, observe_vals, ignore):
            to_return = {
                'require': [],
                'require_not': [],
                'how': 'all',
                'how_not': 'any',
            }

            for ind, val in label_vals.items():
                if val == 0:
                    continue
                if val < 0:
                    to_return['require_not'].append(ind)
                else:
                    to_return['require'].append(ind)

            for ind, val in observe_vals.items():
                if val == 0:
                    to_return['require_not'].append(ind)
                else:
                    to_return['require'].append(ind)

            to_return['require_not'].extend(ignore)
            return to_return

        label_df = [-1, 0, 1]

        temp = [label_df for n in range(len(label_settings))]
        temp.append([0, 1])
        temp.append([-1, 0])
        label_settings = list(label_settings)
        label_settings.extend(observe_settings)
        # Dataframe to dictate should label be -1: left out, 0: observed, 1: required
        label_df = pd.DataFrame(cartesian(temp), columns=label_settings)
        label_df.drop(axis=0, index=label_df.loc[(label_df['keep_cta'] == 0) & (label_df['keep_pet'] == 0)].index, inplace=True)
        label_df.drop(axis=0, index=label_df.loc[(label_df['keep_cta'] == 1) & (label_df['keep_pet'] == -1)].index, inplace=True)

        for _, row in label_df.iterrows():

            observe_df = [0, 1]

            # add the labels with value 0 to the variables to observe
            indices = list()
            for label in row.loc[row == 0].index:
                indices.append(label)
            if len(indices) <= 0:
                continue
            result_df = pd.DataFrame(index=indices, columns=CTA_analyzer.metrics)
            temp = (observe_df for n in range(len(indices)))
            observe_df = pd.DataFrame(cartesian(temp), columns=indices)

            for col in observe_df.columns:

                # gather dataframes for both when col is used and not used
                has_setting = observe_df.loc[observe_df[col] == 1]
                has_no_setting = observe_df.loc[observe_df[col] == 0]

                # iterates through the settings_df, summing together the means of all the dataframes
                # returned by the gen_results
                def sum_means(settings_df):
                    n_files = 0
                    setting_srs = pd.Series(data=0, index=CTA_analyzer.metrics)
                    for _, obs_vals in settings_df.iterrows():
                        kwargs = create_iterator_kwargs(label_vals=row, observe_vals=obs_vals, ignore=list(ignore))
                        dfs = self.gen_results(**kwargs)
                        for df, _ in dfs:
                            n_files += 1
                            setting_srs += df.mean(axis=0)
                    return n_files, setting_srs

                n_setting_files, srs_setting = sum_means(has_setting)
                n_no_setting_files, srs_no_setting = sum_means(has_no_setting)

                srs_setting = srs_setting*n_no_setting_files/n_setting_files
                result_df.loc[col, :] = (srs_setting - srs_no_setting)/(n_setting_files + n_no_setting_files)

            result_df.to_csv(self.output_dir + '\\' + gen_file_name(row))
        pass

    # creates .csv files where each row has the results of the best data selection method
    def get_best_results(self):
        for m in ['Sensitivity', 'Test AUC']:
            all_results = self.gen_results()
            best_results = self.get_best_models(all_results, measure_by=m)
            best_results.to_csv(self.output_dir + '\\' + 'best_results_' + m + '.csv')
        pass

    def PCA(self):
        from CTA_class import CTA_class
        settings = {'pet': False,
                    'cta': True,
                    'basic': True,
                    'only_pet': True,
                    'pca': False
                    }
        n_components = [i for i in range(2, 11)]
        cta_data = CTA_class(**settings)
        data = cta_data.data_fill_na
        for n in n_components:
            pca = PCA(n_components=n)
            pca.fit(data)
            components_mat = pd.DataFrame(data=pca.components_, columns=data.columns)
            explained_variance_ratios = pd.Series(pca.explained_variance_ratio_, dtype='float64')
            components_mat['explained variance ratios'] = explained_variance_ratios
            components_mat.to_csv(self.output_dir + f'\\pca_matrix_{n}_components_only_pet_no_pet.csv')




    @property
    def n_results(self):
        return len(os.listdir(self.results_dir))

if __name__ == '__main__':
    RESULTS_DIR = os.getcwd() + '\\Results'
    OUTPUT_DIR = os.getcwd() + '\\Result analysis'

    to_analyze = {
        #'best_results': CTA_analyzer.get_best_results,
        #'differences': CTA_analyzer.get_setting_differences
        'PCA': CTA_analyzer.PCA
    }
    analyzer = CTA_analyzer(RESULTS_DIR, OUTPUT_DIR, **to_analyze)
    analyzer()
