import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from dataclasses import dataclass
from itertools import product
from sklearn.decomposition import PCA


class CTA_analyzer:
    metrics = ['Test AUC', 'Sensitivity', 'Specificity', 'Accuracy']
    def __init__(self, results_dir, only_pet_dir, output_dir, **analyze_methods):
        self.results_dir = results_dir
        self.only_pet_results_dir = only_pet_dir
        self.output_dir = output_dir
        self.__dict__.update(analyze_methods)
        self.analyze_methods = analyze_methods

    def __call__(self):

        for method_name, method in self.analyze_methods.items():
            method(self)
        pass

    # generates the used label based on the filename
    @staticmethod
    def get_event_from_file_name(file_name):
        label = 'event'
        label = 'cv ' + label if 'cv' in file_name else label
        label = 'timed ' + label if 'timed' in file_name else label
        label = 'early revasc' if 'early_revasc' in file_name else label
        return label

    # Loops through all the results in results_dir and returns them as (dataframe, filename) tuple.
    # If how='any', at least one in "require" must be in filename. Else all of them need to be found.
    # The opposite is true for require_not
    def gen_results(self, require=(), require_not=(), how='any', how_not='any', only_pet=False):

        if isinstance(require, str):
            require = [require]
        if isinstance(require_not, str):
            require_not = [require_not]
        dir = self.only_pet_results_dir if only_pet else self.results_dir
        for f in os.listdir(dir):

            req_list = [n in f for n in require] if len(require) > 0 else [True]
            req_not_list = [n not in f for n in require_not] if len(require_not) > 0 else [True]

            # if how and how_not == any, check if filename has at least one in require and none in require_not
            b_check = max(req_list) if how == 'any' else min(req_list)
            b_check = b_check and min(req_not_list) if how_not == 'any' else b_check and max(req_not_list)
            if b_check:
                yield pd.read_csv(dir + '\\' + f, index_col=0, header=[0,1]), f
        pass

    # Returns the values of gen_result as a dict with filenames as key and dataframe as value.
    def get_results(self, require=(), require_not=(), how='any', how_not='any'):
        to_return = {}
        for df, f in self.gen_results(require=require, require_not=require_not, how=how, how_not=how_not):
            to_return[f] = df
        return to_return

    ########################################################
    ########################################################
    # ANALYZER METHODS
    ########################################################
    ########################################################

    # Returns a dataframe with events on indexes and for columns the metrics
    # of the best model for the label. "Best" is determined by argument measure_by.
    def get_best_models(self, dfs, measure_by='sensitivity'):
        labels = ['event']
        indices = ['all', 'only pet']
        revasc_options = ['all', 'no revasc', 'with revasc']
        multi_columns = pd.MultiIndex.from_product([revasc_options, CTA_analyzer.metrics + ['Model', 'Model Version']])
        to_return = pd.DataFrame(index=indices, columns=multi_columns)
        #to_return['Model'] = ""
        #to_return['Model Version'] = ""
        to_return.loc[:, (slice(None), measure_by)] = -np.inf

        for df, file_name in dfs:
            pet_ind = 'only pet' if 'only_pet' in file_name else 'all'
            for ind, row in df.iterrows():
                revasc_opt = 'all'
                if to_return.at[pet_ind, (revasc_opt, measure_by)] < row.at[(revasc_opt, measure_by)]:
                    to_return.loc[pet_ind, row.index] = row
                    to_return.at[pet_ind, (slice(None), 'Model')] = ind
                    to_return.at[pet_ind, (slice(None), 'Model Version')] = file_name
        return to_return

    # Creates multiple .csv files to self.output_dir
    # Each .csv file contains values a table which shows how each setting in index affects each metric in columns
    # The file names are coded such that only the results in self.results_dir are included that have the same
    # suffixes as the .csv file at hand e.g., if the .csv file generated by this method has 'cv' in its name,
    # that means only the files in self.results_dir were included that also had 'cv' in its name.
    # 'no' keyword flips the requirement for the next keyword (the file MUST NOT have the next
    # keyword in its filename).
    def get_setting_differences(self, label_settings=('cv', 'timed'), observe_settings=('keep_cta', 'keep_pet'),
                                ignore=('only_pet',)):
        from sklearn.utils.extmath import cartesian

        def gen_file_name(label_series):
            to_return = 'settings_differences'

            for ind, val in label_series.items():
                if val == -1:
                    to_return += '_no'
                if abs(val) == 1:
                    to_return += '_' + str(ind)
            return to_return + '.csv'

        # creates kwargs dict for gen_best_models
        def create_iterator_kwargs(label_vals, observe_vals, ignore):
            to_return = {
                'require': [],
                'require_not': [],
                'how': 'all',
                'how_not': 'any',
            }

            for ind, val in label_vals.items():
                if val == 0:
                    continue
                if val < 0:
                    to_return['require_not'].append(ind)
                else:
                    to_return['require'].append(ind)

            for ind, val in observe_vals.items():
                if val == 0:
                    to_return['require_not'].append(ind)
                else:
                    to_return['require'].append(ind)

            to_return['require_not'].extend(ignore)
            return to_return

        label_df = [-1, 0, 1]

        temp = [label_df for n in range(len(label_settings))]
        temp.append([0, 1])
        temp.append([-1, 0])
        label_settings = list(label_settings)
        label_settings.extend(observe_settings)
        # Dataframe to dictate should label be -1: left out, 0: observed, 1: required
        label_df = pd.DataFrame(cartesian(temp), columns=label_settings)
        label_df.drop(axis=0, index=label_df.loc[(label_df['keep_cta'] == 0) & (label_df['keep_pet'] == 0)].index, inplace=True)
        label_df.drop(axis=0, index=label_df.loc[(label_df['keep_cta'] == 1) & (label_df['keep_pet'] == -1)].index, inplace=True)

        for _, row in label_df.iterrows():

            observe_df = [0, 1]

            # add the labels with value 0 to the variables to observe
            indices = list()
            for label in row.loc[row == 0].index:
                indices.append(label)
            if len(indices) <= 0:
                continue
            result_df = pd.DataFrame(index=indices, columns=CTA_analyzer.metrics)
            temp = (observe_df for n in range(len(indices)))
            observe_df = pd.DataFrame(cartesian(temp), columns=indices)

            for col in observe_df.columns:

                # gather dataframes for both when col is used and not used
                has_setting = observe_df.loc[observe_df[col] == 1]
                has_no_setting = observe_df.loc[observe_df[col] == 0]

                # iterates through the settings_df, summing together the means of all the dataframes
                # returned by the gen_results
                def sum_means(settings_df):
                    n_files = 0
                    setting_srs = pd.Series(data=0, index=CTA_analyzer.metrics)
                    for _, obs_vals in settings_df.iterrows():
                        kwargs = create_iterator_kwargs(label_vals=row, observe_vals=obs_vals, ignore=list(ignore))
                        dfs = self.gen_results(**kwargs)
                        for df, _ in dfs:
                            n_files += 1
                            setting_srs += df.mean(axis=0)
                    return n_files, setting_srs

                n_setting_files, srs_setting = sum_means(has_setting)
                n_no_setting_files, srs_no_setting = sum_means(has_no_setting)

                srs_setting = srs_setting*n_no_setting_files/n_setting_files
                result_df.loc[col, :] = (srs_setting - srs_no_setting)/(n_setting_files + n_no_setting_files)

            result_df.to_csv(self.output_dir + '\\' + gen_file_name(row))
        pass

    # creates .csv files where each row has the results of the best data selection method
    def get_best_results(self):
        best_results = None
        for m in [False, True]:
            ind = 'only pet' if m else 'all'
            all_results = self.gen_results(require=[f'{"_only_pet" if m else ""}_pet', "_pca"], only_pet=m)
            temp = self.get_best_models(all_results, measure_by='Test AUC')
            if best_results is None:
                best_results = pd.DataFrame(index=temp.index, columns=temp.columns, dtype='float64')
            best_results.loc[ind, :] = temp.loc[ind, :]
        best_results.to_csv(self.output_dir + '\\' + 'best_results_with_pet.csv')
        pass

    def PCA(self):
        # from CTA_class import CTA_class
        # settings = {'pet': True,
        #             'cta': True,
        #             'basic': True,
        #             'only_pet': False,
        #             'pca': False
        #             }

        n_components = [i for i in range(2, 11)]
        orig_data = pd.read_csv("Training data filled.csv", index_col=0, dtype='float64')
        orig_data = orig_data.iloc[:, :-6]
        pet_data_indices = pd.read_csv("PET Training data filled.csv", index_col=0, dtype='float64').index

        # whether to use the patients with PET data or not
        use_pet_patients = [True, False]
        # whether to use PET data or not
        use_pet_data = [True, False]

        for pet_patients in use_pet_patients:
            temp_data = orig_data.loc[pet_data_indices, :] if pet_patients else orig_data
            for pet_data in use_pet_data:
                data = temp_data.iloc[:, :-20] if not pet_data else temp_data
                for n in n_components:
                    pca = PCA(n_components=n)
                    pca.fit(data)
                    components_mat = pd.DataFrame(data=pca.components_, columns=data.columns)
                    explained_variance_ratios = pd.Series(pca.explained_variance_ratio_, dtype='float64')
                    components_mat['explained variance ratios'] = explained_variance_ratios
                    components_mat.to_csv(self.output_dir + f'\\pca_matrix_{n}_components{"_only_pet" if pet_patients else ""}{"_no_pet" if not pet_data else ""}.csv')




    @property
    def n_results(self):
        return len(os.listdir(self.results_dir))

if __name__ == '__main__':
    RESULTS_DIR = os.getcwd() + '\\Final results'
    ONLY_PET_RESULTS_DIR = os.getcwd() + '\\Final results only pet'
    OUTPUT_DIR = os.getcwd() + '\\Result analysis'

    to_analyze = {
        'best_results': CTA_analyzer.get_best_results,
        #'differences': CTA_analyzer.get_setting_differences
        #'PCA': CTA_analyzer.PCA
    }
    analyzer = CTA_analyzer(RESULTS_DIR, ONLY_PET_RESULTS_DIR, OUTPUT_DIR, **to_analyze)
    analyzer()
